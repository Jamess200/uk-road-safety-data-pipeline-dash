{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400c3363",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO | Data root: c:\\Users\\James\\Documents\\GitHub\\uk-road-safety-data-pipeline-dash\\data\\raw\\dft_road_safety_last_5_years\n",
      "INFO | Detected:\n",
      "- c:\\Users\\James\\Documents\\GitHub\\uk-road-safety-data-pipeline-dash\\data\\raw\\dft_road_safety_last_5_years\\Collisions.csv\n",
      "- c:\\Users\\James\\Documents\\GitHub\\uk-road-safety-data-pipeline-dash\\data\\raw\\dft_road_safety_last_5_years\\Vehicles.csv\n",
      "- c:\\Users\\James\\Documents\\GitHub\\uk-road-safety-data-pipeline-dash\\data\\raw\\dft_road_safety_last_5_years\\Casualties.csv\n",
      "INFO | Collisions: coerced sentinels -> NA (+1528559 new NAs)\n",
      "INFO | Vehicles: coerced sentinels -> NA (+2894042 new NAs)\n",
      "INFO | Casualties: coerced sentinels -> NA (+925886 new NAs)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Pre-merge inspection for the UK DfT Road Safety dataset.\n",
    "\n",
    "WHY THIS SCRIPT EXISTS\n",
    "----------------------\n",
    "Before you join/merge tables or build models, you should *profile* the raw data:\n",
    "- Are the keys unique? (e.g., collision_index)\n",
    "- Do foreign keys exist? (vehicles/casualties -> collisions)\n",
    "- Are there out-of-range values? (lat/long, speed, ages)\n",
    "- What are the shapes, dtypes, nulls, top categories?\n",
    "- Are there duplicates on key combos?\n",
    "- What years are present?\n",
    "\n",
    "WHAT THIS SCRIPT PRODUCES\n",
    "-------------------------\n",
    "It writes three artifacts under 'data/processed/_profile/':\n",
    "\n",
    "1) premerge_profile.md    (human-readable summary you can read & commit)\n",
    "2) premerge_profile.json  (machine-readable profile for CI/tests)\n",
    "3) columns_profile.csv    (column-level stats for diffing over time)\n",
    "\n",
    "HOW TO RUN\n",
    "----------\n",
    "From the repo root:\n",
    "\n",
    "    python scripts/premerge_inspect.py\n",
    "    # Faster test run (reads only first N rows per CSV):\n",
    "    python scripts/premerge_inspect.py --nrows 300000 --emit-samples\n",
    "\n",
    "The script is Jupyter/VS Code friendly (it ignores unknown CLI args).\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.api.types import CategoricalDtype\n",
    "\n",
    "# --------------------------- Configuration ---------------------------\n",
    "\n",
    "# Common \"unknown/not applicable\" sentinel codes in DfT data that we treat as missing\n",
    "SENTINELS = {-1, 97, 98, 99, 997, 998, 999}\n",
    "\n",
    "# Output folder (relative to repo root)\n",
    "OUTDIR_REL = Path(\"data/processed/_profile\")\n",
    "\n",
    "# Display/summary knobs\n",
    "SAMPLE_K = 10   # number of example values to show per column\n",
    "TOP_K = 12      # number of top values to list for categorical columns\n",
    "CSV_SAMPLE_SIZE = 1000  # size of optional sample CSVs\n",
    "\n",
    "# Join contracts (2020+ schema is lower_snake_case)\n",
    "COLLISION_KEY = \"collision_index\"\n",
    "VEH_KEYS = [COLLISION_KEY, \"vehicle_reference\"]\n",
    "CAS_KEYS = [COLLISION_KEY, \"vehicle_reference\", \"casualty_reference\"]\n",
    "\n",
    "# Light sanity checks (bounds). These are not \"truth\", just red-flag detectors.\n",
    "RANGE_RULES = {\n",
    "    \"latitude\": (-90, 90),\n",
    "    \"longitude\": (-180, 180),\n",
    "    \"collision_year\": (2010, 2100),\n",
    "    \"age_of_driver\": (0, 110),\n",
    "    \"age_of_vehicle\": (0, 80),\n",
    "    \"engine_capacity_cc\": (0, 12000),\n",
    "}\n",
    "\n",
    "# Allowed speed limits (mph). We allow a few oddballs like 5/15/80 for edge cases/private roads.\n",
    "ALLOWED_SPEEDS = {5, 10, 15, 20, 30, 40, 50, 60, 70, 80}\n",
    "\n",
    "# Columns typically categorical in this dataset (used for top-values preview)\n",
    "CATEGORICAL_HINTS = [\n",
    "    \"collision_severity\", \"day_of_week\", \"road_type\", \"speed_limit\",\n",
    "    \"weather_conditions\", \"light_conditions\", \"urban_or_rural_area\",\n",
    "    \"vehicle_type\", \"vehicle_manoeuvre\", \"casualty_severity\", \"casualty_type\"\n",
    "]\n",
    "\n",
    "# Expected filenames (no legacy aliases)\n",
    "EXPECTED_FILES = {\n",
    "    \"collisions\": [\"Collisions.csv\"],\n",
    "    \"vehicles\":   [\"Vehicles.csv\"],\n",
    "    \"casualties\": [\"Casualties.csv\"],\n",
    "}\n",
    "\n",
    "# ------------------------------ Utilities ----------------------------\n",
    "\n",
    "def repo_root(start: Path | None = None) -> Path:\n",
    "    \"\"\"Walk upward until we find a folder that looks like a Git repo (has .git or README.md).\"\"\"\n",
    "    cur = start or Path.cwd()\n",
    "    for p in [cur, *cur.parents]:\n",
    "        if (p / \".git\").exists() or (p / \"README.md\").exists():\n",
    "            return p\n",
    "    return cur\n",
    "\n",
    "def find_dataset_root(raw_base: Path) -> Path | None:\n",
    "    \"\"\"\n",
    "    Try to auto-detect the folder that contains the road safety CSVs under data/raw/.\n",
    "    We pick the first folder that either looks like \"road safety\" or contains expected files.\n",
    "    \"\"\"\n",
    "    cands: List[Path] = []\n",
    "    if raw_base.exists():\n",
    "        for d in raw_base.iterdir():\n",
    "            if d.is_dir():\n",
    "                has_expected = any(d.joinpath(n).exists()\n",
    "                                   for names in EXPECTED_FILES.values() for n in names)\n",
    "                looks_like = (\"road\" in d.name.lower() and \"safety\" in d.name.lower()) or has_expected\n",
    "                if looks_like:\n",
    "                    cands.append(d)\n",
    "    return cands[0] if cands else None\n",
    "\n",
    "def find_file(root: Path, names: List[str]) -> Path | None:\n",
    "    \"\"\"Find the first file matching exact name (case-insensitive), otherwise fallback to substring match.\"\"\"\n",
    "    all_csvs = list(root.rglob(\"*.csv\")) + list(root.rglob(\"*.CSV\"))\n",
    "    # exact match first\n",
    "    for nm in names:\n",
    "        for p in all_csvs:\n",
    "            if p.name.lower() == nm.lower():\n",
    "                return p\n",
    "    # substring fallback (e.g., Vehicles_2024.csv)\n",
    "    for nm in names:\n",
    "        stem = nm.lower().replace(\".csv\", \"\")\n",
    "        for p in all_csvs:\n",
    "            if stem in p.name.lower():\n",
    "                return p\n",
    "    return None\n",
    "\n",
    "def read_csv_any(path: Path, nrows: int | None = None) -> pd.DataFrame:\n",
    "    \"\"\"Use pyarrow if available for speed; gracefully fallback otherwise.\"\"\"\n",
    "    try:\n",
    "        return pd.read_csv(path, engine=\"pyarrow\", nrows=nrows)\n",
    "    except Exception:\n",
    "        return pd.read_csv(path, nrows=nrows)\n",
    "\n",
    "def coerce_sentinels_to_na(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Replace common sentinel codes (e.g., -1, 99) with proper NA values on integer/categorical columns.\n",
    "    This prevents silent bugs where \"unknown\" is treated as a real category or value.\n",
    "    \"\"\"\n",
    "    for c in df.columns:\n",
    "        if pd.api.types.is_integer_dtype(df[c]) or isinstance(df[c].dtype, CategoricalDtype):\n",
    "            df[c] = df[c].replace(list(SENTINELS), pd.NA)\n",
    "    return df\n",
    "\n",
    "def json_default(o: Any):\n",
    "    \"\"\"Make numpy/pandas scalars and NA values JSON-serializable.\"\"\"\n",
    "    if o is pd.NA or (isinstance(o, float) and pd.isna(o)) or o is None:\n",
    "        return None\n",
    "    if isinstance(o, (np.integer,)):\n",
    "        return int(o)\n",
    "    if isinstance(o, (np.floating,)):\n",
    "        return float(o)\n",
    "    if isinstance(o, (np.bool_,)):\n",
    "        return bool(o)\n",
    "    return str(o)\n",
    "\n",
    "def top_values(df: pd.DataFrame, col: str, k: int = 12) -> List[Tuple[Any, int]]:\n",
    "    \"\"\"Top-k most frequent values (including NA) for a quick feel of distributions.\"\"\"\n",
    "    if col not in df.columns:\n",
    "        return []\n",
    "    vc = df[col].value_counts(dropna=False).head(k)\n",
    "    out: List[Tuple[Any, int]] = []\n",
    "    for idx, cnt in vc.items():\n",
    "        if idx is pd.NA or (isinstance(idx, float) and pd.isna(idx)):\n",
    "            out.append((None, int(cnt)))\n",
    "        else:\n",
    "            out.append((json_default(idx), int(cnt)))\n",
    "    return out\n",
    "\n",
    "def profile_table(df: pd.DataFrame, name: str) -> Dict[str, Any]:\n",
    "    \"\"\"Column-level overview: dtype, non-null counts, % nulls, uniques, sample values, memory.\"\"\"\n",
    "    info: Dict[str, Any] = {\n",
    "        \"name\": name,\n",
    "        \"shape\": [int(df.shape[0]), int(df.shape[1])],\n",
    "        \"memory_mb\": round(df.memory_usage(deep=True).sum() / 1_048_576, 2),\n",
    "        \"columns\": [],\n",
    "    }\n",
    "    for c in df.columns:\n",
    "        s = df[c]\n",
    "        info[\"columns\"].append({\n",
    "            \"name\": c,\n",
    "            \"dtype\": str(s.dtype),\n",
    "            \"non_null\": int(s.notna().sum()),\n",
    "            \"null_pct\": round(float(s.isna().mean() * 100), 2),\n",
    "            \"nunique\": int(s.nunique(dropna=True)),\n",
    "            \"examples\": s.dropna().astype(str).head(SAMPLE_K).tolist(),\n",
    "        })\n",
    "    return info\n",
    "\n",
    "def check_duplicates(df: pd.DataFrame, keys: List[str]) -> int:\n",
    "    \"\"\"Count rows duplicated on a set of key columns. Returns -1 if any key is missing.\"\"\"\n",
    "    if not all(k in df.columns for k in keys):\n",
    "        return -1\n",
    "    return int(df.duplicated(subset=keys, keep=False).sum())\n",
    "\n",
    "def range_violations(df: pd.DataFrame, col: str, lo: float, hi: float) -> int:\n",
    "    \"\"\"Count values outside [lo, hi] after numeric coercion (non-numeric -> NA -> ignored).\"\"\"\n",
    "    if col not in df.columns:\n",
    "        return 0\n",
    "    s = pd.to_numeric(df[col], errors=\"coerce\").dropna()\n",
    "    return int(((s < lo) | (s > hi)).sum())\n",
    "\n",
    "def membership_violations(df: pd.DataFrame, col: str, allowed: set[int]) -> int:\n",
    "    \"\"\"Count values not in an allowed set (e.g., speed limits).\"\"\"\n",
    "    if col not in df.columns:\n",
    "        return 0\n",
    "    s = pd.to_numeric(df[col], errors=\"coerce\").dropna().astype(int)\n",
    "    return int((~s.isin(allowed)).sum())\n",
    "\n",
    "def write_markdown(md_path: Path, prof: Dict[str, Any], issues: List[str], ranges: Dict[str, Any],\n",
    "                   years: Dict[str, List[int]], cats: Dict[str, Dict[str, List[Tuple[Any, int]]]]):\n",
    "    \"\"\"Render a concise, human-readable report and write it to Markdown.\"\"\"\n",
    "    lines: List[str] = []\n",
    "    lines.append(\"# Pre-merge Inspection Report\\n\")\n",
    "    lines.append(f\"- Data root: `{prof['paths']['data_root']}`\")\n",
    "    lines.append(f\"- Rows × Cols: \"\n",
    "                 f\"Collisions={prof['shapes']['Collisions'][0]:,}×{prof['shapes']['Collisions'][1]}, \"\n",
    "                 f\"Vehicles={prof['shapes']['Vehicles'][0]:,}×{prof['shapes']['Vehicles'][1]}, \"\n",
    "                 f\"Casualties={prof['shapes']['Casualties'][0]:,}×{prof['shapes']['Casualties'][1]}\\n\")\n",
    "\n",
    "    lines.append(\"## Years present\")\n",
    "    for k, v in years.items():\n",
    "        lines.append(f\"- **{k}**: {', '.join(map(str, v))}\")\n",
    "\n",
    "    lines.append(\"\\n## Range checks (out-of-bounds counts)\")\n",
    "    for col, r in ranges.items():\n",
    "        b = r['bounds']\n",
    "        lines.append(f\"- **{col}** [{b[0]}, {b[1]}]: \"\n",
    "                     f\"Collisions={r['Collisions']}, Vehicles={r['Vehicles']}, Casualties={r['Casualties']}\")\n",
    "\n",
    "    lines.append(\"\\n## Key integrity\")\n",
    "    lines.append(\"- Collisions.collision_index unique: \" +\n",
    "                 (\"✅\" if prof['contracts']['collisions_unique'] else \"❌\"))\n",
    "    lines.append(f\"- Vehicles duplicates on {VEH_KEYS}: {prof['contracts']['vehicles_dup']}\")\n",
    "    lines.append(f\"- Casualties duplicates on {CAS_KEYS}: {prof['contracts']['casualties_dup']}\")\n",
    "    lines.append(f\"- Vehicles missing FK -> Collisions: {prof['contracts']['vehicles_missing_fk']}\")\n",
    "    lines.append(f\"- Casualties missing FK -> Collisions: {prof['contracts']['casualties_missing_fk']}\")\n",
    "\n",
    "    if prof['contracts'].get(\"speed_limit_outside_allowed\") is not None:\n",
    "        lines.append(f\"- Speed limits not in {sorted(ALLOWED_SPEEDS)}: \"\n",
    "                     f\"Collisions={prof['contracts']['speed_limit_outside_allowed']['Collisions']}, \"\n",
    "                     f\"Vehicles={prof['contracts']['speed_limit_outside_allowed']['Vehicles']}, \"\n",
    "                     f\"Casualties={prof['contracts']['speed_limit_outside_allowed']['Casualties']}\")\n",
    "\n",
    "    lines.append(\"\\n## Top categorical values (previews)\")\n",
    "    for col, d in cats.items():\n",
    "        lines.append(f\"**{col}**\")\n",
    "        for table, items in d.items():\n",
    "            if items:\n",
    "                preview = \", \".join([f\"{k if k is not None else 'NA'}×{v}\" for k, v in items[:6]])\n",
    "                lines.append(f\"- {table}: {preview}\")\n",
    "\n",
    "    if issues:\n",
    "        lines.append(\"\\n## Issues found\")\n",
    "        for s in issues:\n",
    "            lines.append(f\"- {s}\")\n",
    "    else:\n",
    "        lines.append(\"\\n## Issues found\\n- None detected.\")\n",
    "\n",
    "    md_path.write_text(\"\\n\".join(lines), encoding=\"utf-8\")\n",
    "\n",
    "# --------------------------------- Main ---------------------------------\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"Pre-merge inspection for DfT road safety data\")\n",
    "    parser.add_argument(\"--nrows\", type=int, default=None,\n",
    "                        help=\"Optional row cap per CSV for faster runs (e.g., 300000)\")\n",
    "    parser.add_argument(\"--emit-samples\", action=\"store_true\",\n",
    "                        help=\"Also write small 1k-row CSV samples for each table\")\n",
    "    # JUPYTER/VS CODE SAFE: ignore unknown args like --f=... injected by kernels\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    logging.basicConfig(level=logging.INFO, format=\"%(levelname)s | %(message)s\")\n",
    "\n",
    "    # Discover repo and data location\n",
    "    ROOT = repo_root()\n",
    "    RAW_BASE = ROOT / \"data\" / \"raw\"\n",
    "    data_override = os.getenv(\"UK_RS_DATA_DIR\")  # env var override if data lives elsewhere\n",
    "    DATA_ROOT = Path(data_override) if data_override else find_dataset_root(RAW_BASE)\n",
    "    if not DATA_ROOT or not DATA_ROOT.exists():\n",
    "        raise FileNotFoundError(\n",
    "            f\"Couldn’t find road-safety data under {RAW_BASE}. \"\n",
    "            \"Unzip the DfT files into data/raw/ (e.g., data/raw/dft_road_safety_last_5_years/), \"\n",
    "            \"or set UK_RS_DATA_DIR to the directory.\"\n",
    "        )\n",
    "\n",
    "    # Locate the three expected CSVs\n",
    "    acc_path = find_file(DATA_ROOT, EXPECTED_FILES[\"collisions\"])\n",
    "    veh_path = find_file(DATA_ROOT, EXPECTED_FILES[\"vehicles\"])\n",
    "    cas_path = find_file(DATA_ROOT, EXPECTED_FILES[\"casualties\"])\n",
    "    if not all([acc_path, veh_path, cas_path]):\n",
    "        missing = []\n",
    "        if not acc_path: missing.append(\"Collisions.csv\")\n",
    "        if not veh_path: missing.append(\"Vehicles.csv\")\n",
    "        if not cas_path: missing.append(\"Casualties.csv\")\n",
    "        raise FileNotFoundError(f\"Missing expected tables: {', '.join(missing)}\")\n",
    "\n",
    "    logging.info(f\"Data root: {DATA_ROOT}\")\n",
    "    logging.info(f\"Detected:\\n- {acc_path}\\n- {veh_path}\\n- {cas_path}\")\n",
    "\n",
    "    # Load (optionally capped by --nrows for speed)\n",
    "    acc = read_csv_any(acc_path, nrows=args.nrows)\n",
    "    veh = read_csv_any(veh_path, nrows=args.nrows)\n",
    "    cas = read_csv_any(cas_path, nrows=args.nrows)\n",
    "\n",
    "    # Normalize sentinels to proper NAs (so EDA isn't lying)\n",
    "    for name, df in [(\"Collisions\", acc), (\"Vehicles\", veh), (\"Casualties\", cas)]:\n",
    "        before = df.isna().sum().sum()\n",
    "        df = coerce_sentinels_to_na(df)\n",
    "        after = df.isna().sum().sum()\n",
    "        logging.info(f\"{name}: coerced sentinels -> NA (+{after - before} new NAs)\")\n",
    "        if name == \"Collisions\": acc = df\n",
    "        elif name == \"Vehicles\": veh = df\n",
    "        else: cas = df\n",
    "\n",
    "    # ------------------- Integrity & Contracts -------------------\n",
    "\n",
    "    issues: List[str] = []\n",
    "\n",
    "    # 1) Primary key uniqueness on collisions\n",
    "    collisions_unique = COLLISION_KEY in acc.columns and acc[COLLISION_KEY].is_unique\n",
    "    if not collisions_unique:\n",
    "        dup_n = int(acc.duplicated(subset=[COLLISION_KEY]).sum()) if COLLISION_KEY in acc.columns else -1\n",
    "        issues.append(f\"[Collisions] {COLLISION_KEY} not unique (dups={dup_n})\")\n",
    "\n",
    "    # 2) Foreign key coverage: vehicles/casualties -> collisions\n",
    "    vehicles_missing_fk = 0\n",
    "    casualties_missing_fk = 0\n",
    "    if COLLISION_KEY in veh.columns:\n",
    "        vehicles_missing_fk = int((~veh[COLLISION_KEY].isin(acc[COLLISION_KEY])).sum())\n",
    "        if vehicles_missing_fk:\n",
    "            issues.append(f\"[Vehicles] {vehicles_missing_fk} rows missing FK -> Collisions\")\n",
    "    if COLLISION_KEY in cas.columns:\n",
    "        casualties_missing_fk = int((~cas[COLLISION_KEY].isin(acc[COLLISION_KEY])).sum())\n",
    "        if casualties_missing_fk:\n",
    "            issues.append(f\"[Casualties] {casualties_missing_fk} rows missing FK -> Collisions\")\n",
    "\n",
    "    # 3) Duplicate key combos on vehicles/casualties\n",
    "    vehicles_dup = check_duplicates(veh, VEH_KEYS)\n",
    "    casualties_dup = check_duplicates(cas, CAS_KEYS)\n",
    "    if vehicles_dup > 0:\n",
    "        issues.append(f\"[Vehicles] duplicate rows on {VEH_KEYS}: {vehicles_dup}\")\n",
    "    if casualties_dup > 0:\n",
    "        issues.append(f\"[Casualties] duplicate rows on {CAS_KEYS}: {casualties_dup}\")\n",
    "\n",
    "    # ------------------- Range & Domain Checks -------------------\n",
    "\n",
    "    # 4) Ranges (bounds)\n",
    "    range_report: Dict[str, Dict[str, Any]] = {}\n",
    "    for col, (lo, hi) in RANGE_RULES.items():\n",
    "        range_report[col] = {\n",
    "            \"Collisions\": range_violations(acc, col, lo, hi),\n",
    "            \"Vehicles\":   range_violations(veh, col, lo, hi),\n",
    "            \"Casualties\": range_violations(cas, col, lo, hi),\n",
    "            \"bounds\": [lo, hi],\n",
    "        }\n",
    "\n",
    "    # 5) Allowed values for speed_limit\n",
    "    speed_out = {\n",
    "        \"Collisions\": membership_violations(acc, \"speed_limit\", ALLOWED_SPEEDS),\n",
    "        \"Vehicles\":   membership_violations(veh, \"speed_limit\", ALLOWED_SPEEDS),\n",
    "        \"Casualties\": membership_violations(cas, \"speed_limit\", ALLOWED_SPEEDS),\n",
    "    }\n",
    "\n",
    "    # 6) Years present (helps pick train/test windows & detect partial years)\n",
    "    years: Dict[str, List[int]] = {}\n",
    "    for name, df in [(\"Collisions\", acc), (\"Vehicles\", veh), (\"Casualties\", cas)]:\n",
    "        if \"collision_year\" in df.columns:\n",
    "            yrs = pd.to_numeric(df[\"collision_year\"], errors=\"coerce\").dropna().astype(int).unique()\n",
    "            years[name] = sorted(map(int, yrs))\n",
    "\n",
    "    # 7) Top values for common categorical columns (quick distribution snapshots)\n",
    "    cats: Dict[str, Dict[str, List[Tuple[Any, int]]]] = {}\n",
    "    for col in CATEGORICAL_HINTS:\n",
    "        cats[col] = {\n",
    "            \"Collisions\": top_values(acc, col),\n",
    "            \"Vehicles\":   top_values(veh, col),\n",
    "            \"Casualties\": top_values(cas, col),\n",
    "        }\n",
    "\n",
    "    # ------------------- Column Profile CSV -------------------\n",
    "\n",
    "    OUTDIR = ROOT / OUTDIR_REL\n",
    "    OUTDIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    def cols_df(df: pd.DataFrame, table: str) -> pd.DataFrame:\n",
    "        rows = []\n",
    "        for c in df.columns:\n",
    "            s = df[c]\n",
    "            rows.append({\n",
    "                \"table\": table,\n",
    "                \"column\": c,\n",
    "                \"dtype\": str(s.dtype),\n",
    "                \"non_null\": int(s.notna().sum()),\n",
    "                \"null_pct\": float(s.isna().mean() * 100),\n",
    "                \"nunique\": int(s.nunique(dropna=True)),\n",
    "                \"memory_bytes\": int(s.memory_usage(deep=True)),\n",
    "            })\n",
    "        return pd.DataFrame(rows)\n",
    "\n",
    "    cols_profile = pd.concat([\n",
    "        cols_df(acc, \"Collisions\"),\n",
    "        cols_df(veh, \"Vehicles\"),\n",
    "        cols_df(cas, \"Casualties\")\n",
    "    ], ignore_index=True)\n",
    "    cols_profile.sort_values([\"table\", \"column\"]).to_csv(OUTDIR / \"columns_profile.csv\", index=False)\n",
    "\n",
    "    # ------------------- Assemble & Write Reports -------------------\n",
    "\n",
    "    prof = {\n",
    "        \"paths\": {\n",
    "            \"root\": str(ROOT),\n",
    "            \"data_root\": str(DATA_ROOT),\n",
    "            \"collisions\": str(acc_path),\n",
    "            \"vehicles\": str(veh_path),\n",
    "            \"casualties\": str(cas_path),\n",
    "        },\n",
    "        \"shapes\": {\n",
    "            \"Collisions\": [int(acc.shape[0]), int(acc.shape[1])],\n",
    "            \"Vehicles\":   [int(veh.shape[0]), int(veh.shape[1])],\n",
    "            \"Casualties\": [int(cas.shape[0]), int(cas.shape[1])],\n",
    "        },\n",
    "        \"profiles\": {\n",
    "            \"Collisions\": profile_table(acc, \"Collisions\"),\n",
    "            \"Vehicles\":   profile_table(veh, \"Vehicles\"),\n",
    "            \"Casualties\": profile_table(cas, \"Casualties\"),\n",
    "        },\n",
    "        \"ranges\": range_report,\n",
    "        \"categories_top\": cats,\n",
    "        \"years_present\": years,\n",
    "        \"contracts\": {\n",
    "            \"collisions_unique\": bool(collisions_unique),\n",
    "            \"vehicles_dup\": int(vehicles_dup),\n",
    "            \"casualties_dup\": int(casualties_dup),\n",
    "            \"vehicles_missing_fk\": int(vehicles_missing_fk),\n",
    "            \"casualties_missing_fk\": int(casualties_missing_fk),\n",
    "            \"speed_limit_outside_allowed\": speed_out,\n",
    "        },\n",
    "        \"issues\": issues,\n",
    "        \"run\": {\n",
    "            \"nrows\": args.nrows,\n",
    "            \"emit_samples\": bool(args.emit_samples),\n",
    "        }\n",
    "    }\n",
    "\n",
    "    json_path = OUTDIR / \"premerge_profile.json\"\n",
    "    md_path = OUTDIR / \"premerge_profile.md\"\n",
    "\n",
    "    with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(prof, f, indent=2, default=json_default)\n",
    "\n",
    "    write_markdown(md_path, prof, issues, range_report, years, cats)\n",
    "\n",
    "    # Optional: write small human-friendly samples for each table\n",
    "    if args.emit_samples:\n",
    "        (OUTDIR / \"samples\").mkdir(exist_ok=True, parents=True)\n",
    "        for name, df in [(\"collisions\", acc), (\"vehicles\", veh), (\"casualties\", cas)]:\n",
    "            df.head(CSV_SAMPLE_SIZE).to_csv(OUTDIR / \"samples\" / f\"{name}_sample_{CSV_SAMPLE_SIZE}.csv\", index=False)\n",
    "\n",
    "    logging.info(f\"Wrote:\\n- {md_path}\\n- {json_path}\\n- {OUTDIR/'columns_profile.csv'}\")\n",
    "    if args.emit_samples:\n",
    "        logging.info(f\"- samples: {OUTDIR/'samples'}/*_sample_{CSV_SAMPLE_SIZE}.csv\")\n",
    "\n",
    "    # CI NOTE:\n",
    "    # To fail CI when issues are detected, uncomment the next two lines.\n",
    "    # if issues:\n",
    "    #     raise SystemExit(1)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
